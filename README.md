# Retail Data Pipeline (E-Commerce Dataset)

*A complete end-to-end data engineering portfolio project.*

This project demonstrates a real-world data pipeline built on a synthetic e-commerce dataset containing users, products, orders, order_items, reviews, and event logs. It covers ingestion, cleaning, transformation, warehouse modeling, data quality testing, analytics, and orchestration with Airflow.

---

## ğŸš€ Architecture

![Architecture Diagram](docs/architecture.png)

---

## ğŸ“‚ Project Structure

```text
retail-data-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # original CSV files
â”‚   â”œâ”€â”€ clean/               # cleaned ingestion outputs
â”‚   â””â”€â”€ warehouse/           # dim_* and fact_* tables
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest/              # ingestion code
â”‚   â”œâ”€â”€ transform/           # transformation code
â”‚   â””â”€â”€ load/                # (optional) loading scripts
â”‚
â”œâ”€â”€ dags/                    # Airflow DAGs
â”œâ”€â”€ sql/                     # SQL schema + business queries
â”œâ”€â”€ tests/                   # pytest data quality tests
â”œâ”€â”€ docs/                    # architecture diagrams, notes
â””â”€â”€ README.md
```

## ğŸ§± Pipeline Stages

### 1ï¸âƒ£ Ingestion

**File:** `src/ingest/ingest_data.py`

Reads 6 raw CSV files from `data/raw`:

- users.csv
- products.csv
- orders.csv
- order_items.csv
- reviews.csv
- events.csv

- Removes completely empty rows  
- Saves cleaned versions to `data/clean/` (prefixed with `clean_`)

---

### 2ï¸âƒ£ Transformation

**File:** `src/transform/transform_data.py`

Loads cleaned data from `data/clean/`

Builds warehouse tables:

- dim_users  
- dim_products  
- fact_sales (joins orders + order_items)

Additional processing:

- Normalizes `order_status` values  
- Computes `line_amount`  
- Saves warehouse tables to `data/warehouse/`

---

### 3ï¸âƒ£ Data Warehouse

**Location:** `data/warehouse/`

Contains:

- dim_users.csv  
- dim_products.csv  
- fact_sales.csv  

---

### 4ï¸âƒ£ Data Quality Testing

**Files:**

- `tests/test_transformations.py`  
- `tests/test_data_quality.py`

Tests include:

- No null `user_id` or `product_id`  
- No negative or zero quantities  
- Correct `line_amount` calculations  
- Valid gender values  
- Non-negative product prices  
- Only allowed `order_status` values  

Run tests:

```bash
pytest
```

---

### 5ï¸âƒ£ SQL Analytics

Folder: `sql/`

Includes:

- `create_tables.sql` â€“ warehouse schema documentation
- `business_metrics.sql` â€“ example business queries

Queries cover:

- Revenue by category
- Top-selling products
- Daily revenue trends
- Customer lifetime value
- Order status distribution

---

### 6ï¸âƒ£ Orchestration with Airflow

File: `dags/retail_pipeline_dag.py`

A daily Airflow DAG that runs:

Ingest â†’ Transform â†’ Warehouse

Uses `PythonOperator` to call pipeline scripts.

---

## ğŸ›  Tech Stack

- Python (Pandas)
- SQL
- Pytest
- Apache Airflow (DAG authoring)
- Git & GitHub
- VS Code
- Powerpoint (architecture diagram)

---

##â–¶ï¸ How to Run This Project Locally

###1. Clone repository

```bash
git clone https://github.com/DeepthiPalle381/retail-data-pipeline.git
cd retail-data-pipeline

```

###2. Create virtual environment

```bash
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt

```

###3. Run ingestion

```bash
python src/ingest/ingest_data.py

```

###4. Run transformation

```bash
python src/transform/transform_data.py

```

###5. Run tests

```bash
pytest


